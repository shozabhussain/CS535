{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 5 - Neural Networks.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6_DLRhh7zXA"
      },
      "source": [
        "# Assignment 5 - Neural Networks\n",
        "\n",
        "\n",
        "#### Roll Number: xxxxxxxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyusByDc74A0"
      },
      "source": [
        "### Task Explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEDxZ7d875RE"
      },
      "source": [
        "In this assignment, we will see how changes in the neural network architecture and the hyperparameters can have an effect on how the network performs \n",
        "\n",
        "Wheat rust is a devastating plant disease that affects many crops, reducing yields and affecting the livelihoods of farmers and decreasing food security across the continent. The disease is difficult to monitor at a large scale, making it difficult to control and eradicate.\n",
        "\n",
        "The objective of this challenge is to see how small changes in the model architecture and the hyperparameters can have an effect on the final results. You are required to add screenshots of the results (the accuracy, history and loss graphs and the confusion matrix) with the changes you made in the architecture and the hyperparameters in a pdf file, and submit it along with the code.\n",
        "\n",
        "ALL THE CODE HAS ALREADY BEEN WRITTEN FOR YOU. YOU ARE ONLY REQUIRED TO MAKE CHANGES IN THE CODE WHERE IT IS ASKED OF YOU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi0LGVvZ8For"
      },
      "source": [
        "### Let's Start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTzuPmgx8HBc"
      },
      "source": [
        "Make necessary imports here e.g. import cv2, import glob, etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j63bSz-_7qG_"
      },
      "source": [
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Dropout, BatchNormalization, Input, LeakyReLU, Flatten, Dropout, ReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import Model\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import keras\n",
        "import random\n",
        "from keras.datasets import mnist\n",
        "# any other imports that you may require"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sjo9fKM98Mh8"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_duJq-FH86Mu"
      },
      "source": [
        "CHANGE THESE ONLY WHEN YOU ARE ASKED TO LATER ON IN THE ASSIGNMENT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hV3BwFQ8Pwl"
      },
      "source": [
        "batch_size = 32\n",
        "epochs = 30\n",
        "learning_rate = 0.0001\n",
        "input_shape = (256,256,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnIcmW5R8WH_"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzDuqfJT8XVN"
      },
      "source": [
        "MAKE NO CHANGES IN THIS PART"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f8zmUug8TGD"
      },
      "source": [
        "!git clone https://github.com/MMFa666/WheatDiseaseDataset.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKdAUxVj8fop"
      },
      "source": [
        "healthy_wheat = '/content/WheatDiseaseDataset/train/healthy_wheat'\n",
        "leaf_rust = '/content/WheatDiseaseDataset/train/leaf_rust'\n",
        "stem_rust = '/content/WheatDiseaseDataset/train/stem_rust'\n",
        "healthy_wheat_files = glob.glob(healthy_wheat + '/*.jpg')\n",
        "leaf_rust_files = glob.glob(leaf_rust + '/*.jpg')\n",
        "stem_rust_files = glob.glob(stem_rust + '/*.jpg')\n",
        "print(len(healthy_wheat_files))\n",
        "print(len(leaf_rust_files))\n",
        "print(len(stem_rust_files))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm-3N94D8hiH"
      },
      "source": [
        "train_dir = '/content/WheatDiseaseDataset/train'\n",
        "train_files = glob.glob(train_dir + '/*/*.jpg')\n",
        "len(train_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWXz_BYn8lGM"
      },
      "source": [
        "x = train_files[0].split('/')\n",
        "x[4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8wbdodb8n2O"
      },
      "source": [
        "test_dir = '/content/WheatDiseaseDataset/test'\n",
        "test_files = glob.glob(test_dir + '/*/*.jpg')\n",
        "len(test_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nImqz6ME8sYc"
      },
      "source": [
        "### Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br8DmE5o8vXH"
      },
      "source": [
        "MAKE NO CHANGES IN THIS PART"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-39MpVH8q6j"
      },
      "source": [
        "labels={}\n",
        "labels['healthy_wheat'] = 0\n",
        "labels['leaf_rust'] = 1\n",
        "labels['stem_rust'] = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYahBSJp80_e"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3xcUZ979gRt"
      },
      "source": [
        "MAKE NO CHANGES IN THIS PART"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYOOSV6I827u"
      },
      "source": [
        "def preprocessing_norm(images):\n",
        "    return images/255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V70tMYZk9ixs"
      },
      "source": [
        "### Batch Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71wiqtN19nNw"
      },
      "source": [
        "MAKE NO CHANGES IN THIS PART"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hF1woJ9S9kzO"
      },
      "source": [
        "def parse_path(filename):\n",
        "  one_hot = [0,0,0]\n",
        "  x = filename.split('/')\n",
        "  lab = x[4]\n",
        "  label = labels[lab]\n",
        "  one_hot[label] = one_hot[label] + 1\n",
        "  return one_hot\n",
        "\n",
        "def get_image(filename):\n",
        "  img = cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2RGB)\n",
        "  image = cv2.resize(img, (256,256))\n",
        "  return image\n",
        "\n",
        "def data_generator(data, batch_size):\n",
        "  total_size = len(data)\n",
        "  indexes = np.arange(0, total_size, batch_size)\n",
        "  if total_size % batch_size != 0:\n",
        "    indexes = indexes[:-1]  \n",
        "  while True:\n",
        "    np.random.shuffle(indexes)\n",
        "    for index in indexes:\n",
        "      batch_paths = data[index:index+batch_size]\n",
        "      batch_x = np.array([get_image(path) for path in batch_paths])\n",
        "      batch_y = np.array([parse_path(path) for path in batch_paths])\n",
        "      #batch_x = preprocessing_norm(batch_x)\n",
        "      yield batch_x, batch_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtJRcEt4-LiN"
      },
      "source": [
        "Initialize train data generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzqBr75E-MTk"
      },
      "source": [
        "data = data_generator(train_files, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7la8cZaP-Odx"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55PhKZ77-UKU"
      },
      "source": [
        "THE BASE MODEL HAS ALREADY BEEN BUILT FOR YOU. INITIALIZE AND COMPILE THIS MODEL AND SAVE THE SCREENSHOTS OF THE RESULTS IN THE PDF. \n",
        "\n",
        "YOU WILL BE MAKING THE CHANGES TO THE ARCHITECTURE IN THIS CELL (SPECIFIED LATER IN THE ASSIGNMENT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBVwuwMe-QZD"
      },
      "source": [
        "input_img = Input(shape=(input_shape))\n",
        "l = Conv2D(256, kernel_size=(5,5),strides=(2,2))(input_img)\n",
        "l = BatchNormalization()(l)\n",
        "l = LeakyReLU()(l)\n",
        "l = Dropout(0.2)(l)\n",
        "l = Conv2D(128, kernel_size=(3,3),strides=(2,2))(l)\n",
        "l = BatchNormalization()(l)\n",
        "l = LeakyReLU()(l)\n",
        "l = Dropout(0.2)(l)\n",
        "l = Conv2D(64, kernel_size=(3,3),strides=(2,2))(l)\n",
        "l = BatchNormalization()(l)\n",
        "l = LeakyReLU()(l)\n",
        "l = Dropout(0.2)(l)\n",
        "l = Conv2D(32, kernel_size=(3,3),strides=(2,2))(l)\n",
        "l = BatchNormalization()(l)\n",
        "l = LeakyReLU()(l)\n",
        "l = Dropout(0.2)(l)\n",
        "l = Conv2D(16, kernel_size=(3,3),strides=(2,2))(l)\n",
        "l = BatchNormalization()(l)\n",
        "l = LeakyReLU()(l)\n",
        "l = Dropout(0.2)(l)\n",
        "l = Flatten()(l)\n",
        "l = Dense(128)(l)\n",
        "l = LeakyReLU()(l)\n",
        "l = Dropout(0.2)(l)\n",
        "l = Dense(32)(l)\n",
        "l = LeakyReLU()(l)\n",
        "l = Dropout(0.2)(l)\n",
        "output_class = Dense(3, activation='softmax')(l)\n",
        "\n",
        "model_initial = Model(inputs=input_img, outputs=output_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_xFicXS-e4T"
      },
      "source": [
        "#print model summary here\n",
        "model_initial.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srUZeknV-jVc"
      },
      "source": [
        "adam = Adam(lr = learning_rate)\n",
        "model_initial.compile(loss='categorical_crossentropy', \n",
        "              optimizer=adam, \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIaC5Jfb-mvU"
      },
      "source": [
        "hist = model_initial.fit(data, batch_size = batch_size, epochs=epochs, steps_per_epoch=21)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziyg6ag0-pnl"
      },
      "source": [
        "Plot the loss and accuracy graphs of training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0DCzHGm-qUb"
      },
      "source": [
        "# Loss Plot\n",
        "plt.plot(hist.history['loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU8tm7c6-uQf"
      },
      "source": [
        "# Accuracy Plot\n",
        "plt.plot(hist.history['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAME5E_q-yKM"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np3mhYYa-18t"
      },
      "source": [
        "Now, we will evaluate our model on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4NDIc7X-w01"
      },
      "source": [
        "test_data = data_generator(test_files, 177)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ii-kih2-5Xt"
      },
      "source": [
        "predictions = model_initial.predict(next(test_data)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lghMxLgC-7AE"
      },
      "source": [
        "pred = []\n",
        "truth = []\n",
        "for prediction in predictions:\n",
        "  pred.append(np.argmax(prediction))\n",
        "for true in next(test_data)[1]:\n",
        "  truth.append(np.argmax(true))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YerdbR1--rU"
      },
      "source": [
        "Calculate and print accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao0Nlv1X-9jM"
      },
      "source": [
        "true = 0\n",
        "total = len(pred)\n",
        "for i in range(total):\n",
        "  if pred[i] == truth[i]:\n",
        "    true = true + 1\n",
        "print(\"Accuracy =\", (true/total)*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVcW9UXD_DLN"
      },
      "source": [
        "Calculate and print Confusion Matrix. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-YbU_KW_FFb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxoKWR_k_SRt"
      },
      "source": [
        "def plot_confusion_matrix(conf_mat):\n",
        "    classes = list(labels.keys())\n",
        "    df_cm = pd.DataFrame(conf_mat,classes,classes)\n",
        "    plt.figure(figsize=(10,7))\n",
        "    sns.set(font_scale=1.4)\n",
        "    sns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16})\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w7a2UdCWJ3Q"
      },
      "source": [
        "Use the above function to plot confusion matrix here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1Y6KPEW_b5b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m29EU7mZVrD3"
      },
      "source": [
        "### Changes to be made\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxetUutBW9Px"
      },
      "source": [
        "Now that you've successfuly ran and gotten the results for the model that we built for you, make the following changes (one by one), and see how they effect the performance and the results of the model. In total, you will have the results of 5 different experiments (including those of the network already built for you)\n",
        "\n",
        "SCREENSHOT ALL THE RESULTS (INCLUDING THOSE FOR THE MODEL WE BUILT FOR YOU) AND ADD THEM TO A PDF. \n",
        "\n",
        "Change #1: Change the learning rate to 0.01. <br>\n",
        "Change #2: Change the epochs to 50 (make sure to change the learning rate back to 0.0001) <br>\n",
        "Change #3: Change the Leaky ReLU activation to ReLU (make sure to use 30 epochs and a learning rate of 0.0001) <br>\n",
        "Change #4: Increase the depth of the neural network by adding another convolutional layer (followed by the batch normalization layer, the leaky ReLU activation layer, and the dropout layer). This new piece of code will be added right before the convolution layer with the filter size of 32 in the architecture (line 14). The code to be added is as follows:     \n",
        "```\n",
        "l = Conv2D(64, kernel_size=(3,3),strides=(2,2))(l) \n",
        "l = BatchNormalization()(l) \n",
        "l = LeakyReLU()(l) \n",
        "l = Dropout(0.2)(l) \n",
        "```\n",
        "Again, this is to be added in the 14th line, i.e, between the following two lines of code in the architecture:     \n",
        "```\n",
        "l = Dropout(0.2)(l)\n",
        "l = Conv2D(32, kernel_size=(3,3),strides=(2,2))(l)\n",
        "```\n",
        "You will notice that this is the exact layer collection as the previous layer. Make sure to change all the ReLU layers back to Leaky ReLU and keep the epochs at 30 and the learning rate at 0.0001 for a controlled experiment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QeB-aefW64T"
      },
      "source": [
        "### Questions (You might need to do research)\n",
        "\n",
        "1) How did changing the learning rate impact your results? Why? \n",
        "\n",
        "\n",
        "2) How did changing the number of epochs impact your results? Why? \n",
        "\n",
        "\n",
        "3) How did changing the leaky relu function to a normal relu function impact your results? Why?\n",
        "\n",
        "4) How did adding a new layer group impact your results? Why?\n",
        "\n",
        "4) The batch normalization and the dropout layers act like regularization layers. Theoretically, what would happen if all these regularization layers are removed?\n",
        "\n",
        "5) What is class imbalance? Is there class imbalance in this problem? Why or why not?"
      ]
    }
  ]
}